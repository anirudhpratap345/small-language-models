{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "849a8204",
   "metadata": {},
   "source": [
    "# Diving Deep into Small Language Models\n",
    "## Understanding the Architecture and Mechanics of SLMs\n",
    "\n",
    "This notebook explores the fundamental workings of small language models (SLMs), including:\n",
    "- **Tokenization & Embeddings**: Converting text to numerical representations\n",
    "- **Transformer Architecture**: The backbone of modern language models\n",
    "- **Attention Mechanisms**: How models focus on relevant information\n",
    "- **Text Generation**: Autoregressive decoding and sampling strategies\n",
    "- **Evaluation Metrics**: Assessing model performance\n",
    "\n",
    "Let's begin our journey!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820a90aa",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "We'll use transformers, PyTorch, and visualization tools to explore SLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8325f29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GPT2Tokenizer\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87531d1",
   "metadata": {},
   "source": [
    "## 2. Tokenization and Embeddings\n",
    "\n",
    "**Tokenization** is the process of converting text into tokens (subword units).\n",
    "**Embeddings** map tokens to dense numerical vectors.\n",
    "\n",
    "Let's see how this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdda4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Sample text\n",
    "sample_text = \"Small language models are efficient and powerful!\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = tokenizer.encode(sample_text)\n",
    "token_strings = tokenizer.convert_ids_to_tokens(tokens)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TOKENIZATION PROCESS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Original text: {sample_text}\")\n",
    "print(f\"\\nTokens (IDs): {tokens}\")\n",
    "print(f\"\\nToken strings: {token_strings}\")\n",
    "print(f\"Number of tokens: {len(tokens)}\")\n",
    "print(f\"Number of unique tokens: {len(set(tokens))}\")\n",
    "\n",
    "# Create a vocabulary size overview\n",
    "vocab_size = tokenizer.vocab_size\n",
    "print(f\"\\nGPT-2 vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f19b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the tokenization process\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "\n",
    "# Create a visual representation of tokens\n",
    "for i, (token_id, token_str) in enumerate(zip(tokens, token_strings)):\n",
    "    ax.barh(i, 1, left=0, height=0.8, color=plt.cm.viridis(token_id/vocab_size))\n",
    "    ax.text(0.5, i, f\"{token_str} ({token_id})\", va='center', ha='center', \n",
    "            color='white', fontweight='bold', fontsize=9)\n",
    "\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(-0.5, len(tokens)-0.5)\n",
    "ax.set_ylabel('Token Position', fontsize=11)\n",
    "ax.set_title('Tokenization Process: Text to Token IDs', fontsize=13, fontweight='bold')\n",
    "ax.set_xticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Each token is mapped to a unique integer ID in the vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ae11e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Layer Demonstration\n",
    "# Create a simple embedding layer\n",
    "embedding_dim = 8  # Small dimension for visualization\n",
    "embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "# Convert token IDs to embeddings\n",
    "token_tensor = torch.tensor(tokens)\n",
    "embeddings = embedding_layer(token_tensor)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EMBEDDINGS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Token IDs shape: {token_tensor.shape}\")\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"Embedding dimension: {embedding_dim}\")\n",
    "\n",
    "# Show first token embedding\n",
    "print(f\"\\nFirst token '{token_strings[0]}' embedding vector:\")\n",
    "print(embeddings[0].detach().numpy())\n",
    "\n",
    "# Visualize embeddings as a heatmap\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "im = ax.imshow(embeddings.detach().numpy(), aspect='auto', cmap='coolwarm')\n",
    "ax.set_xlabel('Embedding Dimension', fontsize=11)\n",
    "ax.set_ylabel('Token Position', fontsize=11)\n",
    "ax.set_title(f'Token Embeddings Heatmap (dim={embedding_dim})', fontsize=13, fontweight='bold')\n",
    "ax.set_yticks(range(len(token_strings)))\n",
    "ax.set_yticklabels(token_strings, fontsize=9)\n",
    "plt.colorbar(im, ax=ax, label='Value')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Each token is now represented as a dense vector in embedding space\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bf01f6",
   "metadata": {},
   "source": [
    "## 3. Transformer Architecture Basics\n",
    "\n",
    "The **Transformer** architecture, introduced in \"Attention is All You Need\" (Vaswani et al., 2017), consists of:\n",
    "1. **Positional Encoding**: Adds position information to embeddings\n",
    "2. **Multi-Head Attention**: Allows the model to attend to different representation subspaces\n",
    "3. **Feed-Forward Network**: Non-linear transformations\n",
    "4. **Layer Normalization**: Stabilizes training\n",
    "5. **Residual Connections**: Enables deep networks\n",
    "\n",
    "Let's build a simplified transformer block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5835e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=100):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # Sine and cosine functions with different frequencies\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                            -(np.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "# Create positional encoding\n",
    "d_model = 32\n",
    "pos_encoder = PositionalEncoding(d_model, max_len=20)\n",
    "\n",
    "# Visualize positional encoding\n",
    "pos_encoding = pos_encoder.pe[:10, :]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Heatmap of positional encoding\n",
    "im = axes[0].imshow(pos_encoding.T, aspect='auto', cmap='RdBu')\n",
    "axes[0].set_xlabel('Position', fontsize=11)\n",
    "axes[0].set_ylabel('Dimension', fontsize=11)\n",
    "axes[0].set_title('Positional Encoding Pattern', fontsize=12, fontweight='bold')\n",
    "plt.colorbar(im, ax=axes[0])\n",
    "\n",
    "# Pattern visualization\n",
    "for i in range(4):\n",
    "    axes[1].plot(pos_encoding[:, i].numpy(), label=f'Dim {i}', marker='o')\n",
    "axes[1].set_xlabel('Position', fontsize=11)\n",
    "axes[1].set_ylabel('Encoding Value', fontsize=11)\n",
    "axes[1].set_title('Positional Encoding Values', fontsize=12, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Positional encoding provides position information to the model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79461573",
   "metadata": {},
   "source": [
    "## 4. Attention Mechanism\n",
    "\n",
    "**Self-Attention** allows each token to attend to all other tokens in the sequence.\n",
    "\n",
    "The attention score is computed as:\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "Where:\n",
    "- **Q (Query)**: What the token is looking for\n",
    "- **K (Key)**: What information each token has\n",
    "- **V (Value)**: The actual information to pass\n",
    "\n",
    "Let's implement and visualize self-attention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b97c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "        self.fc_out = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_k)\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        context = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        # Reshape back and apply final linear layer\n",
    "        context = context.transpose(1, 2).contiguous()\n",
    "        context = context.view(batch_size, seq_len, self.d_model)\n",
    "        output = self.fc_out(context)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Create sample input\n",
    "batch_size = 1\n",
    "seq_len = 5\n",
    "d_model = 16\n",
    "\n",
    "sample_input = torch.randn(batch_size, seq_len, d_model)\n",
    "attention = SelfAttention(d_model, num_heads=2)\n",
    "\n",
    "output, attn_weights = attention(sample_input)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SELF-ATTENTION MECHANISM\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Input shape: {sample_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "print(f\"Number of attention heads: 2\")\n",
    "\n",
    "# Visualize attention weights\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "for head_idx in range(2):\n",
    "    attn_map = attn_weights[0, head_idx].detach().numpy()\n",
    "    im = axes[head_idx].imshow(attn_map, cmap='YlOrRd', vmin=0, vmax=1)\n",
    "    axes[head_idx].set_xlabel('Key Position', fontsize=10)\n",
    "    axes[head_idx].set_ylabel('Query Position', fontsize=10)\n",
    "    axes[head_idx].set_title(f'Attention Weights - Head {head_idx + 1}', fontweight='bold')\n",
    "    plt.colorbar(im, ax=axes[head_idx])\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(seq_len):\n",
    "        for j in range(seq_len):\n",
    "            axes[head_idx].text(j, i, f'{attn_map[i, j]:.2f}', \n",
    "                              ha='center', va='center', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Attention weights show how each position attends to all other positions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afea3eba",
   "metadata": {},
   "source": [
    "## 5. Forward Pass Through a Small Language Model\n",
    "\n",
    "Let's trace how data flows through a complete model. We'll use DistilGPT-2, a distilled (smaller) version of GPT-2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d71a66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a small pre-trained model\n",
    "print(\"Loading DistilGPT-2 model...\")\n",
    "model_name = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, output_hidden_states=True)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Prepare input\n",
    "text = \"Machine learning is\"\n",
    "input_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FORWARD PASS ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Input text: {text}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Input shape: {input_ids.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, return_dict=True)\n",
    "\n",
    "logits = outputs.logits\n",
    "hidden_states = outputs.hidden_states\n",
    "\n",
    "print(f\"\\nModel outputs:\")\n",
    "print(f\"  Logits shape: {logits.shape}\")\n",
    "print(f\"  Number of hidden layers: {len(hidden_states)}\")\n",
    "print(f\"  Hidden state shape: {hidden_states[0].shape}\")\n",
    "\n",
    "# Show how logits transform to probabilities\n",
    "next_token_logits = logits[0, -1, :]\n",
    "next_token_probs = F.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "# Get top-5 predictions\n",
    "top_k = 5\n",
    "top_probs, top_indices = torch.topk(next_token_probs, top_k)\n",
    "\n",
    "print(f\"\\nTop-{top_k} next token predictions:\")\n",
    "for i, (prob, idx) in enumerate(zip(top_probs, top_indices), 1):\n",
    "    token_str = tokenizer.decode([idx])\n",
    "    print(f\"  {i}. {token_str:20s} - Probability: {prob.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c233fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the flow through layers\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "\n",
    "# 1. Logits distribution\n",
    "axes[0, 0].hist(logits[0, -1, :].detach().cpu().numpy(), bins=50, color='skyblue', edgecolor='black')\n",
    "axes[0, 0].set_xlabel('Logit Value', fontsize=10)\n",
    "axes[0, 0].set_ylabel('Frequency', fontsize=10)\n",
    "axes[0, 0].set_title('Distribution of Logits for Next Token', fontweight='bold')\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# 2. Probability distribution (top 20)\n",
    "top_20_probs, top_20_indices = torch.topk(next_token_probs, 20)\n",
    "token_strings = [tokenizer.decode([idx]).strip() or f\"[{idx}]\" for idx in top_20_indices]\n",
    "axes[0, 1].barh(range(20), top_20_probs.detach().cpu().numpy(), color='coral')\n",
    "axes[0, 1].set_yticks(range(20))\n",
    "axes[0, 1].set_yticklabels(token_strings, fontsize=9)\n",
    "axes[0, 1].set_xlabel('Probability', fontsize=10)\n",
    "axes[0, 1].set_title('Top-20 Next Token Predictions', fontweight='bold')\n",
    "axes[0, 1].invert_yaxis()\n",
    "axes[0, 1].grid(alpha=0.3, axis='x')\n",
    "\n",
    "# 3. Hidden state magnitudes across layers\n",
    "layer_magnitudes = []\n",
    "for hidden in hidden_states:\n",
    "    magnitude = torch.norm(hidden[0, -1, :]).item()\n",
    "    layer_magnitudes.append(magnitude)\n",
    "\n",
    "axes[1, 0].plot(layer_magnitudes, marker='o', linewidth=2, markersize=8, color='green')\n",
    "axes[1, 0].set_xlabel('Layer', fontsize=10)\n",
    "axes[1, 0].set_ylabel('Hidden State Magnitude', fontsize=10)\n",
    "axes[1, 0].set_title('Hidden State Magnitudes Across Layers', fontweight='bold')\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# 4. Token embedding progression\n",
    "token_embeddings = []\n",
    "for hidden in hidden_states:\n",
    "    embedding = hidden[0, -1, :].detach().cpu().numpy()\n",
    "    token_embeddings.append(embedding[:32])  # First 32 dimensions\n",
    "\n",
    "token_embeddings = np.array(token_embeddings)\n",
    "im = axes[1, 1].imshow(token_embeddings, aspect='auto', cmap='coolwarm')\n",
    "axes[1, 1].set_xlabel('Embedding Dimension', fontsize=10)\n",
    "axes[1, 1].set_ylabel('Layer', fontsize=10)\n",
    "axes[1, 1].set_title('Token Embedding Evolution Across Layers', fontweight='bold')\n",
    "plt.colorbar(im, ax=axes[1, 1], label='Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Data flows through embedding ‚Üí attention layers ‚Üí MLP ‚Üí output projections\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97afddc1",
   "metadata": {},
   "source": [
    "## 6. Temperature and Sampling in Generation\n",
    "\n",
    "**Temperature** controls the randomness of predictions:\n",
    "- **Low temperature (< 1.0)**: Model makes confident, deterministic choices\n",
    "- **High temperature (> 1.0)**: Model becomes more creative and unpredictable\n",
    "\n",
    "Different sampling strategies:\n",
    "1. **Greedy Decoding**: Always pick the most likely token\n",
    "2. **Top-K Sampling**: Sample from the K most likely tokens\n",
    "3. **Nucleus (Top-P) Sampling**: Sample from tokens with cumulative probability ‚â§ P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c50473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate temperature effects\n",
    "temperatures = [0.3, 0.7, 1.0, 1.5, 2.0]\n",
    "original_probs = F.softmax(logits[0, -1, :], dim=-1).detach().cpu().numpy()\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(16, 4))\n",
    "\n",
    "for idx, temp in enumerate(temperatures):\n",
    "    # Apply temperature\n",
    "    scaled_logits = logits[0, -1, :] / temp\n",
    "    temp_probs = F.softmax(scaled_logits, dim=-1).detach().cpu().numpy()\n",
    "    \n",
    "    # Get top tokens\n",
    "    top_indices = np.argsort(-temp_probs)[:10]\n",
    "    top_probs_vals = temp_probs[top_indices]\n",
    "    token_labels = [tokenizer.decode([i]).strip() or f\"[{i}]\" for i in top_indices]\n",
    "    \n",
    "    # Plot\n",
    "    axes[idx].barh(range(10), top_probs_vals, color=plt.cm.viridis(idx/len(temperatures)))\n",
    "    axes[idx].set_yticks(range(10))\n",
    "    axes[idx].set_yticklabels(token_labels, fontsize=8)\n",
    "    axes[idx].set_xlim(0, max(top_probs_vals) * 1.1)\n",
    "    axes[idx].set_title(f'T={temp}', fontweight='bold')\n",
    "    axes[idx].set_xlabel('Probability', fontsize=9)\n",
    "    axes[idx].invert_yaxis()\n",
    "    if idx == 0:\n",
    "        axes[idx].set_ylabel('Tokens', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Effect of Temperature on Probability Distribution', \n",
    "             fontsize=13, fontweight='bold', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TEMPERATURE EFFECTS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Low Temperature (0.3):   Sharp distribution, deterministic\")\n",
    "print(\"High Temperature (2.0):  Flat distribution, more random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80baedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate different sampling strategies\n",
    "def top_k_sampling(logits, k=5, temperature=1.0):\n",
    "    \"\"\"Select from top-k most likely tokens\"\"\"\n",
    "    logits = logits / temperature\n",
    "    top_k_logits, top_k_indices = torch.topk(logits, k)\n",
    "    probs = F.softmax(top_k_logits, dim=-1)\n",
    "    return probs, top_k_indices\n",
    "\n",
    "def top_p_sampling(logits, p=0.9, temperature=1.0):\n",
    "    \"\"\"Select from smallest set of tokens with cumulative probability >= p\"\"\"\n",
    "    logits = logits / temperature\n",
    "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "    sorted_probs = F.softmax(sorted_logits, dim=-1)\n",
    "    cumsum_probs = torch.cumsum(sorted_probs, dim=0)\n",
    "    \n",
    "    # Remove tokens above cumulative probability threshold\n",
    "    sorted_indices_to_remove = cumsum_probs > p\n",
    "    sorted_indices_to_remove[0] = False  # Keep at least one token\n",
    "    \n",
    "    sorted_probs[sorted_indices_to_remove] = 0\n",
    "    sorted_probs = sorted_probs / sorted_probs.sum()\n",
    "    \n",
    "    return sorted_probs, sorted_indices\n",
    "\n",
    "# Visualize sampling strategies\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# All tokens\n",
    "all_probs = F.softmax(logits[0, -1, :], dim=-1).detach().cpu().numpy()\n",
    "top_indices_all = np.argsort(-all_probs)[:15]\n",
    "top_probs_all = all_probs[top_indices_all]\n",
    "token_labels = [tokenizer.decode([i]).strip() or f\"[{i}]\" for i in top_indices_all]\n",
    "\n",
    "axes[0].barh(range(15), top_probs_all, color='skyblue')\n",
    "axes[0].set_yticks(range(15))\n",
    "axes[0].set_yticklabels(token_labels, fontsize=9)\n",
    "axes[0].set_title('Greedy Decoding\\n(All tokens considered)', fontweight='bold')\n",
    "axes[0].set_xlabel('Probability', fontsize=10)\n",
    "axes[0].invert_yaxis()\n",
    "axes[0].grid(alpha=0.3, axis='x')\n",
    "\n",
    "# Top-K sampling\n",
    "top_k_probs, top_k_idx = top_k_sampling(logits[0, -1, :], k=5)\n",
    "top_k_probs = top_k_probs.detach().cpu().numpy()\n",
    "top_k_idx = top_k_idx.detach().cpu().numpy()\n",
    "token_labels_k = [tokenizer.decode([i]).strip() or f\"[{i}]\" for i in top_k_idx]\n",
    "\n",
    "axes[1].barh(range(5), top_k_probs, color='lightcoral')\n",
    "axes[1].set_yticks(range(5))\n",
    "axes[1].set_yticklabels(token_labels_k, fontsize=9)\n",
    "axes[1].set_title('Top-K Sampling (K=5)\\n(Only 5 best tokens)', fontweight='bold')\n",
    "axes[1].set_xlabel('Probability', fontsize=10)\n",
    "axes[1].invert_yaxis()\n",
    "axes[1].grid(alpha=0.3, axis='x')\n",
    "\n",
    "# Top-P sampling\n",
    "top_p_probs, top_p_idx = top_p_sampling(logits[0, -1, :], p=0.9)\n",
    "top_p_probs = top_p_probs.detach().cpu().numpy()\n",
    "top_p_idx_nonzero = top_p_idx[top_p_probs > 0]\n",
    "top_p_probs_nonzero = top_p_probs[top_p_probs > 0]\n",
    "token_labels_p = [tokenizer.decode([i]).strip() or f\"[{i}]\" for i in top_p_idx_nonzero]\n",
    "\n",
    "axes[2].barh(range(len(token_labels_p)), top_p_probs_nonzero, color='lightgreen')\n",
    "axes[2].set_yticks(range(len(token_labels_p)))\n",
    "axes[2].set_yticklabels(token_labels_p, fontsize=9)\n",
    "axes[2].set_title(f'Nucleus Sampling (P=0.9)\\n({len(token_labels_p)} tokens)', fontweight='bold')\n",
    "axes[2].set_xlabel('Probability', fontsize=10)\n",
    "axes[2].invert_yaxis()\n",
    "axes[2].grid(alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Different sampling strategies balance quality and diversity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6d5861",
   "metadata": {},
   "source": [
    "## 7. Inference and Text Generation\n",
    "\n",
    "Let's generate text using different strategies and parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7ab2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text generation with different parameters\n",
    "prompts = [\n",
    "    \"Artificial intelligence is\",\n",
    "    \"The future of technology will\",\n",
    "    \"Deep learning models are\"\n",
    "]\n",
    "\n",
    "generation_configs = {\n",
    "    \"Greedy\": {\n",
    "        \"do_sample\": False,\n",
    "        \"max_new_tokens\": 20,\n",
    "        \"temperature\": 1.0\n",
    "    },\n",
    "    \"Temperature=0.5\": {\n",
    "        \"do_sample\": True,\n",
    "        \"max_new_tokens\": 20,\n",
    "        \"temperature\": 0.5,\n",
    "        \"top_p\": 0.95\n",
    "    },\n",
    "    \"Temperature=1.5\": {\n",
    "        \"do_sample\": True,\n",
    "        \"max_new_tokens\": 20,\n",
    "        \"temperature\": 1.5,\n",
    "        \"top_p\": 0.95\n",
    "    },\n",
    "    \"Top-K=5\": {\n",
    "        \"do_sample\": True,\n",
    "        \"max_new_tokens\": 20,\n",
    "        \"temperature\": 1.0,\n",
    "        \"top_k\": 5\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TEXT GENERATION EXAMPLES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"\\nüìù Prompt: {prompt}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    for config_name, config in generation_configs.items():\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(\n",
    "                input_ids,\n",
    "                **config,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        generated_continuation = generated_text[len(prompt):]\n",
    "        \n",
    "        print(f\"  [{config_name:15s}]: {generated_continuation}\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35b725d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize generation process step-by-step\n",
    "prompt = \"Machine learning is\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP-BY-STEP GENERATION PROCESS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "generated_sequence = input_ids.clone()\n",
    "token_sequence = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n",
    "probs_sequence = []\n",
    "entropy_sequence = []\n",
    "\n",
    "for step in range(5):  # Generate 5 tokens\n",
    "    with torch.no_grad():\n",
    "        outputs = model(generated_sequence)\n",
    "    \n",
    "    next_token_logits = outputs.logits[0, -1, :]\n",
    "    next_token_probs = F.softmax(next_token_logits, dim=-1)\n",
    "    \n",
    "    # Calculate entropy\n",
    "    entropy = -(next_token_probs * torch.log(next_token_probs + 1e-10)).sum()\n",
    "    entropy_sequence.append(entropy.item())\n",
    "    \n",
    "    # Sample next token\n",
    "    next_token = torch.multinomial(next_token_probs, num_samples=1)\n",
    "    generated_sequence = torch.cat([generated_sequence, next_token.unsqueeze(0)], dim=1)\n",
    "    \n",
    "    token_str = tokenizer.decode(next_token.item())\n",
    "    token_sequence.append(token_str)\n",
    "    \n",
    "    top_prob = next_token_probs.max().item()\n",
    "    probs_sequence.append(top_prob)\n",
    "    \n",
    "    full_text = tokenizer.decode(generated_sequence[0], skip_special_tokens=True)\n",
    "    print(f\"Step {step+1}: Added '{token_str:10s}' | Max prob: {top_prob:.4f} | Text: {full_text}\")\n",
    "\n",
    "# Visualize the process\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Max probability over steps\n",
    "steps = list(range(1, 6))\n",
    "axes[0].plot(steps, probs_sequence, marker='o', linewidth=2, markersize=8, color='blue')\n",
    "axes[0].set_xlabel('Generation Step', fontsize=11)\n",
    "axes[0].set_ylabel('Max Token Probability', fontsize=11)\n",
    "axes[0].set_title('Model Confidence During Generation', fontweight='bold')\n",
    "axes[0].set_ylim(0, 1)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Entropy over steps\n",
    "axes[1].plot(steps, entropy_sequence, marker='s', linewidth=2, markersize=8, color='red')\n",
    "axes[1].set_xlabel('Generation Step', fontsize=11)\n",
    "axes[1].set_ylabel('Entropy of Distribution', fontsize=11)\n",
    "axes[1].set_title('Distribution Uncertainty During Generation', fontweight='bold')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Generation is an autoregressive process: each new token depends on all previous tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61646a5f",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation Metrics\n",
    "\n",
    "Two key metrics for evaluating language models:\n",
    "- **Perplexity**: How surprised the model is by the next token (lower is better)\n",
    "- **Log-Likelihood**: The sum of log probabilities (higher is better)\n",
    "\n",
    "$$\\text{Perplexity} = e^{-\\frac{1}{N} \\sum_{i=1}^{N} \\log P(x_i)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0f21f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on different texts\n",
    "evaluation_texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Machine learning is a subset of artificial intelligence.\",\n",
    "    \"Natural language processing enables computers to understand human language.\",\n",
    "    \"Python is a popular programming language for data science.\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL EVALUATION METRICS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results = []\n",
    "\n",
    "for text in evaluation_texts:\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "    \n",
    "    loss = outputs.loss.item()\n",
    "    perplexity = np.exp(loss)\n",
    "    \n",
    "    results.append({\n",
    "        'text': text,\n",
    "        'loss': loss,\n",
    "        'perplexity': perplexity,\n",
    "        'length': input_ids.shape[1]\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nText: {text[:50]}...\")\n",
    "    print(f\"  Loss:       {loss:.4f}\")\n",
    "    print(f\"  Perplexity: {perplexity:.4f}\")\n",
    "    print(f\"  Seq Length: {input_ids.shape[1]}\")\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "texts_short = [r['text'][:40] + \"...\" for r in results]\n",
    "losses = [r['loss'] for r in results]\n",
    "perplexities = [r['perplexity'] for r in results]\n",
    "\n",
    "# Loss\n",
    "axes[0].bar(range(len(texts_short)), losses, color='lightblue', edgecolor='black')\n",
    "axes[0].set_ylabel('Loss', fontsize=11)\n",
    "axes[0].set_title('Language Modeling Loss', fontweight='bold')\n",
    "axes[0].set_xticks(range(len(texts_short)))\n",
    "axes[0].set_xticklabels(texts_short, rotation=45, ha='right', fontsize=9)\n",
    "axes[0].grid(alpha=0.3, axis='y')\n",
    "\n",
    "# Perplexity\n",
    "axes[1].bar(range(len(texts_short)), perplexities, color='lightcoral', edgecolor='black')\n",
    "axes[1].set_ylabel('Perplexity', fontsize=11)\n",
    "axes[1].set_title('Perplexity Score', fontweight='bold')\n",
    "axes[1].set_xticks(range(len(texts_short)))\n",
    "axes[1].set_xticklabels(texts_short, rotation=45, ha='right', fontsize=9)\n",
    "axes[1].grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Lower perplexity indicates the model better predicts the text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb7de0c",
   "metadata": {},
   "source": [
    "## Key Takeaways: Understanding Small Language Models\n",
    "\n",
    "### üéØ Architecture\n",
    "1. **Tokenization**: Text ‚Üí Token IDs\n",
    "2. **Embeddings**: Token IDs ‚Üí Dense vectors\n",
    "3. **Positional Encoding**: Add position information\n",
    "4. **Transformer Layers**: Multi-head attention + Feed-forward networks\n",
    "5. **Language Modeling Head**: Predict next token\n",
    "\n",
    "### üß† Key Mechanisms\n",
    "- **Self-Attention**: Each token can focus on any other token in context\n",
    "- **Multi-Head Attention**: Process information from different representation subspaces\n",
    "- **Layer Normalization**: Stabilizes training\n",
    "- **Residual Connections**: Enables deeper networks\n",
    "\n",
    "### üé≤ Generation Strategies\n",
    "- **Greedy**: Always pick highest probability (deterministic)\n",
    "- **Temperature**: Scale probabilities (0.5 = confident, 1.5 = creative)\n",
    "- **Top-K**: Sample from K most likely tokens\n",
    "- **Nucleus (Top-P)**: Sample from cumulative probability ‚â§ P\n",
    "\n",
    "### üìä Evaluation\n",
    "- **Perplexity**: Exponential of average negative log-likelihood\n",
    "- **Lower is better**: Model is less surprised by the data\n",
    "\n",
    "### ‚ö° Small Language Models Advantages\n",
    "‚úì Faster inference  \n",
    "‚úì Lower computational requirements  \n",
    "‚úì Can run on edge devices  \n",
    "‚úì Easier to fine-tune  \n",
    "‚úì More interpretable for research\n",
    "\n",
    "---\n",
    "\n",
    "**Next Steps**: Explore fine-tuning, prompt engineering, or more advanced techniques like quantization and distillation!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
